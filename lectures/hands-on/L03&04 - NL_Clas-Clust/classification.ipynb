{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"10ea4f999fef4e97913fa6efc7262597","deepnote_cell_type":"markdown"},"source":"# Hands-on: Classification","block_group":"fcfd8d3f965747b7ae6b3fcb40f3b765"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"c1778026a902401294a996582d36dc72","deepnote_cell_type":"code"},"source":"# Basic inputs\nimport numpy as np\nfrom numpy import mean\nfrom numpy import std\n\nfrom scipy import linalg\nfrom scipy import stats\n\n# Drawing modules \nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib import colors\nfrom matplotlib.colors import ListedColormap, LinearSegmentedColormap\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Rectangle\nfrom matplotlib.patches import Ellipse\n\n# Create color maps\ncmap_light_b = ListedColormap([\"#FFAAAA\", \"#AAFFAA\", \"#AAAAFF\"])\ncmap_light = ListedColormap([\"#FFAAAA\", \"#AAAAFF\", \"#AAFFAA\"])\n#cmap_light = ListedColormap(['red', 'blue', 'green'])\ncmap_bold = ListedColormap([\"#FF0000\", \"#0000FF\", \"#00FF00\"])\n\nmy_cmap= ListedColormap(['#ff3d3d', '#3d3dff', '#2dc22d', '#fcea19'])\n\ncolors = [\"red\",\"blue\"]\ncmap1 = LinearSegmentedColormap.from_list(\"mycmap\", colors)\n\n# Create color dictionary for classes\ncolor_class = { 0 : 'red'\n               ,1 : 'blue'\n               ,2 : 'green'\n               ,3 : 'yellow'\n            }","block_group":"cc9e5ebd066d410786a001b6b4742aa9","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"c7b4cfa8584147b9a6e585ca9cef42dd","deepnote_cell_type":"code"},"source":"# Data creation : Gaussian samples with fixed covariances\n# a bit misterious but not too much\n\ndef dataset_fixed_cov(n=300,dim=2):\n    \"\"\"Generate 2 Gaussians samples with the same covariance matrix\"\"\"\n    #n, dim = 300, 2\n    np.random.seed(0)\n    # centers\n    C = np.array([[0.0, -0.23], [0.83, 0.23]])\n    X = np.r_[\n        np.dot(np.random.randn(n, dim), C),\n        np.dot(np.random.randn(n, dim), C) + np.array([1, 1]),\n    ]\n    y = np.hstack((np.zeros(n), np.ones(n)))\n    return X, y\n\n\ndef dataset_cov():\n    \"\"\"Generate 2 Gaussians samples with different covariance matrices\"\"\"\n    n, dim = 300, 2\n    np.random.seed(0)\n    C = np.array([[0.0, -1.0], [2.5, 0.7]]) * 2.0\n    X = np.r_[\n        np.dot(np.random.randn(n, dim), C),\n        np.dot(np.random.randn(n, dim), C.T) + np.array([1, 4]),\n    ]\n    y = np.hstack((np.zeros(n), np.ones(n)))\n    return X, y\n\ndef Test_model_accuracy (y_pred, y_test):\n    \n    dif =  (y_pred - y_test)==0\n    count=np.unique(dif, return_counts=True)\n\n    accuracy = count[1][1]/len(y_test) \n    print ('Accuracy (direct calculation): ', accuracy) \n\n    Conf_Mat = confusion_matrix(y_test, y_pred)\n    tn, fp, fn, tp = Conf_Mat.ravel()\n\n    print('True positive: ', tp)\n    print('True negative: ', tn)\n    print('False positive: ', fp)\n    print('False negative: ', fn)\n\n    print ('Accuracy (Conf. Matrix): ', (tp+tn)/(tp+tn+fp+fn)) \n    prec = tp/(tp+fp)\n    print ('Precision (Conf. Matrix): ', prec)\n    rec = tp/(tp+fn)\n    print ('Recall (Conf. Matrix): ', rec) \n    f1s = 2*(rec*prec/(rec+prec))\n    print ('F1-score (Conf. Matrix): ', f1s) \n    print ('F1-score (skl metrics): ', f1_score(y_test, y_pred)) \n","block_group":"06f4a01df8934f77add6e53e0aaf0808","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"ed940f58d0cb45e584ba1eeeeacfdc58","deepnote_cell_type":"markdown"},"source":"# Manage the data","block_group":"c65f4729fe224918b318ca95d84d3136"},{"cell_type":"markdown","metadata":{"cell_id":"197bbbe247cc453e81d1c8133d501f22","deepnote_cell_type":"markdown"},"source":"## Get the data ","block_group":"3794ac39bb52435e9335b3e3e1b7ee15"},{"cell_type":"code","metadata":{"cell_id":"4b354cc576ff4e689f110485b041383d","deepnote_cell_type":"code"},"source":"# Create or load the data\nN_sample = 300\nN_dim =2\n\nX,y = dataset_fixed_cov(N_sample,N_dim)","block_group":"7f7e8b92d2c34e40be13b52f26194e98","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"98cfc7d9f6e24548bc38583a48e19de5","deepnote_cell_type":"markdown"},"source":"## Visualize the data","block_group":"a66e295377f744e6ade33b50de69ab42"},{"cell_type":"code","metadata":{"cell_id":"3d08009ceb3b47b4bf99f80bd5b9c448","deepnote_cell_type":"code"},"source":"Figure, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\nax1.scatter(X[:,0],X[:,1])\nax1.set_xlabel(r'$x_1$',fontsize = 16)\nax1.set_ylabel(r'$x_2$',fontsize = 16)\n\nX0_s, X1_s = X[y == 0], X[y == 1]\nax2.scatter(X0_s[:,0],X0_s[:,1], marker = '.', color = color_class[0], alpha=0.5)\nax2.scatter(X1_s[:,0],X1_s[:,1], marker = '^', color = color_class[1], alpha=0.5)\nax2.set_xlabel(r'$x_1$',fontsize = 16)\nax2.set_ylabel(r'$x_2$',fontsize = 16)","block_group":"3e452850dcef4217b5add3f24ffbf0ba","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"83e552725462405cace3976d1e4abe03","deepnote_cell_type":"markdown"},"source":"## Train-test split","block_group":"d07dfd31e1d14ade856c5135baa1bae0"},{"cell_type":"code","metadata":{"cell_id":"58f250c953d24537bc20c28be003b215","deepnote_cell_type":"code"},"source":"# Here we use now the train test split of Scikit-learn\nfrom sklearn.model_selection import train_test_split\n\n# Define a random state for reproducibility during the different use and methods\nrandom_state = 42\n\n# split the dataset into a train set and a test set : usually 80% of the data in the training\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=random_state)","block_group":"7040c2c73ece46b5b985fe4c77f93daa","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"b6b44a5aed964e5db8557ef36efebdbc","deepnote_cell_type":"markdown"},"source":"### Visualize the train and test with the classes","block_group":"001e86bf059b4f40bbd19ba31c44cef1"},{"cell_type":"code","metadata":{"cell_id":"d2364baff42f4c6f963e2209ada73515","deepnote_cell_type":"code"},"source":"Figure, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\nax1.scatter(X[:,0],X[:,1], marker = 'o', s = 30)\nax1.scatter(X_train[:,0],X_train[:,1], marker = '.', s = 10, color='red')\nax1.set_xlabel(r'$x_1$',fontsize = 16)\nax1.set_ylabel(r'$x_2$',fontsize = 16)\n\n# separate instances with classes 0 and 1 for the drawing\nX0_tr, X1_tr = X_train[y_train == 0], X_train[y_train == 1]\n\nax2.scatter(X0_tr[:,0],X0_tr[:,1], marker = '.', color = color_class[0])\nax2.scatter(X1_tr[:,0],X1_tr[:,1], marker = '^', color = color_class[1])\nax2.set_title('Train set',fontsize = 16)\nax2.set_xlabel(r'$x_1$',fontsize = 16)\nax2.set_ylabel(r'$x_2$',fontsize = 16)","block_group":"c3a2f25b47f04cfdbe50684692b353dd","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"d7156faab5c24528915689ed8e73c5fa","deepnote_cell_type":"code"},"source":"Figure, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\nax1.scatter(X[:,0],X[:,1], marker = 'o', s = 30)\nax1.scatter(X_test[:,0],X_test[:,1], marker = '.', s = 10, color='red')\nax1.set_xlabel(r'$x_1$',fontsize = 16)\nax1.set_ylabel(r'$x_2$',fontsize = 16)\n\n# separate instances with classes 0 and 1 for the drawing\nX0_t, X1_t = X_test[y_test == 0], X_test[y_test == 1]\n\n\nax2.scatter(X0_t[:,0],X0_t[:,1], marker = '.', color='red')\nax2.scatter(X1_t[:,0],X1_t[:,1], marker = '^', color='blue')\nax2.set_title('Test set',fontsize = 16)\nax2.set_xlabel(r'$x_1$',fontsize = 16)\nax2.set_ylabel(r'$x_2$',fontsize = 16)","block_group":"c310c2cc8536487bb01cdf384c0be92d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"50a58bf51b9d48a08e268e29d8fd2e36","deepnote_cell_type":"markdown"},"source":"## Linear discriminant analysis (LDA)\n\nIn a first step LDA is used to analyze the dataset in each of the two components $x_1$ and $x_2$ to see how the classes separate on each of them. \n\nIn particular, we will  \n* see how they spread on $x_1$ and $x_2$,\n* calculate their cendroids\n* Calculate the bivariate correlation and represent them\n","block_group":"197dbf0b141e4c1bb8a1a413bf71b14b"},{"cell_type":"code","metadata":{"cell_id":"0eb72c37d5b540ac8b308ddd8d8faf03","deepnote_cell_type":"code"},"source":"# Here is the classifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis","block_group":"36005c1328944746a1e10dc431742b0a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"281a5d8f42884ea9882dd439b1dfa7d5","deepnote_cell_type":"markdown"},"source":"### Train the model","block_group":"89330bc94b334151b1d63f4ae9db3bcf"},{"cell_type":"code","metadata":{"cell_id":"2b1e6d5423d944bda694e763a50dd9dc","deepnote_cell_type":"code"},"source":"# create the model\nlda = LinearDiscriminantAnalysis(solver=\"svd\", store_covariance=True)\nmodel_lda = lda.fit(X_train, y_train)\n\ny_pred = model_lda.predict(X_test)\n\nprint('Centroids:\\n',model_lda.means_)\nprint('Covariance matrix:\\n',model_lda.covariance_)","block_group":"3bc7113aa49d439d836b07793abd0d19","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"97ba7bd89e19467285508b8cb8f6cf11","deepnote_cell_type":"code"},"source":"# Plot the centroids and covariance ellipses\n\nFigure, ax2 = plt.subplots(nrows=1, ncols=1, figsize=(7,5))\n\n# plot the dataset separated in two classes\n#\nX0_s, X1_s = X[y == 0], X[y == 1]\nax2.scatter(X0_s[:,0],X0_s[:,1], marker = '.', color = 'red', alpha=0.1)\nax2.scatter(X1_s[:,0],X1_s[:,1], marker = '^', color = 'blue', alpha=0.1)\nax2.set_xlabel(r'$x_1$',fontsize = 16)\nax2.set_ylabel(r'$x_2$',fontsize = 16)\n\n# Plot the centroids\n#\nax2.plot(\n   model_lda.means_[0][0],\n   model_lda.means_[0][1],\n   \"*\",\n   color=\"yellow\",\n   markersize=15,\n   markeredgecolor=\"grey\",\n)\nax2.plot(\n   model_lda.means_[1][0],\n   model_lda.means_[1][1],\n   \"*\",\n   color=\"yellow\",\n   markersize=15,\n   markeredgecolor=\"grey\",\n)\n\n# The eigenvalues of the covatiance matrix are calculated \n# Determine the pricipal vectors of the bivariates distributions\n# This is similar to the principal component analysis\n#  \nv, w = linalg.eigh(model_lda.covariance_)\n\n# For class 0\nu = w[0] / linalg.norm(w[0])\nangle = np.arctan(u[1] / u[0])\nangle = 180 * angle / np.pi  # convert to degrees\n\n# For class 1\nu2 = w[1] / linalg.norm(w[1])\nangle = np.arctan(u2[1] / u2[0])\nangle = 180 * angle / np.pi  # convert to degrees\n\n# fill the aera corresponding to a Gaussian inside 2 times the standard deviation\n# This is a filled Ellipse\nell0 = Ellipse(\n    lda.means_[0,:],\n    2 * v[0] ** 0.5,\n    2 * v[1] ** 0.5,\n    angle = 90 + angle,\n    facecolor='red',\n    edgecolor=\"red\",\n    linewidth=3,\n)\n\nell1 = Ellipse(\n    lda.means_[1,:],\n    2 * v[0] ** 0.5,\n    2 * v[1] ** 0.5,\n    angle = 90 + angle,\n    facecolor='blue',\n    edgecolor=\"blue\",\n    linewidth=3,\n)\n\nell0.set_clip_box(ax2.bbox)\nell0.set_alpha(0.3)\nell1.set_clip_box(ax2.bbox)\nell1.set_alpha(0.3)\nax2.add_artist(ell0)\nax2.add_artist(ell1)\n\n#Plot the distribution on each axis\nfact = 0.5\noffs = -1\nx =  np.linspace(-2.5,3,200)\ny1 = fact*stats.norm.pdf(x, loc= model_lda.means_[0][0], scale=2*v[0]**0.5)\ny2 = fact*stats.norm.pdf(x, loc= model_lda.means_[1][0], scale=2*v[0]**0.5)\n\nfact2 = 0.5\noffs2 = -2.5\nx2 =  np.linspace(-1,2,200)\ny12 = fact2*stats.norm.pdf(x2, loc= model_lda.means_[0][1], scale=v[0]**0.5)\ny22 = fact*stats.norm.pdf(x2, loc= model_lda.means_[1][1], scale=v[0]**0.5)\nax2.plot(x, offs+y1+y2, color='black', linestyle = 'dashed')\nax2.plot(x, offs+y1, color='red', alpha = 0.6)\nax2.plot(x, offs+y2, color='blue', alpha = 0.6)\n\nax2.plot(offs2+y12+y22, x2, color='black', linestyle = 'dashed')\nax2.plot(offs2+y12, x2, color='red', alpha = 0.6)\nax2.plot(offs2+y22, x2, color='blue', alpha = 0.6)\n#ax2.plot(x,offs+y1+y2,color='black',linestyle='-',alpha = 0.5)\n\n\n#ax2.set_xticks(())\n#ax2.set_yticks(())\n","block_group":"0ae4894ed9d94eb084c7a80e8e86461d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"e9b0176f1c124f61a9c81bd6300c792f","deepnote_cell_type":"markdown"},"source":"### Analysis\n* From the gaussian distributions used to represent the data, $x_1$ and $x_2$ are both representative of the $2$ classes \n* The classes are more separated on axis $x_2$, we can imagine that the line of separation will be on this axix ","block_group":"241d8d0db97c434989a9bf2d9d2d6060"},{"cell_type":"markdown","metadata":{"cell_id":"773810f847b842719e812f65b193688e","deepnote_cell_type":"markdown"},"source":"# Binary classification","block_group":"91dd1ff2dba245bd9fb3d4a1b9ffab96"},{"cell_type":"code","metadata":{"cell_id":"dfd8c5a188904e88bec17afbe4c22abf","deepnote_cell_type":"code"},"source":"# Import some tools from Scikit-Learn\n\n# Metrix\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\n\n# Inspection tools\nfrom sklearn.inspection import DecisionBoundaryDisplay","block_group":"297dca20949e475590f385110039d836","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"e04bff631e3c4d3cb9fe1401dcd629d6","deepnote_cell_type":"markdown"},"source":"## Logistic regression","block_group":"08ed5b933d154d25bfddeca6fcf46994"},{"cell_type":"code","metadata":{"cell_id":"8da021fcac8340b38471d54a5f931c97","deepnote_cell_type":"code"},"source":"from sklearn.linear_model import LogisticRegression","block_group":"23a7fae7b871497bb1b13857918d18a1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"6636908c4b69407fba3d43d8fe331bd8","deepnote_cell_type":"code"},"source":"# Create the model for binary classification\n# 'liblinear' and 'ovr' are chosen here for binary logistic regression. \n# Please note: for multinomial logistic regression, you can use 'lbfgs' and 'multinomial'.\n\nlogreg = LogisticRegression(solver='liblinear', multi_class='ovr') ","block_group":"e43950fff9454855b3e53078a4116f82","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"3550b569994a4885a89284b9f15dff0c","deepnote_cell_type":"code"},"source":"logreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nproba_log = logreg.predict_proba(X_test)\n\nTest_model_accuracy (y_pred, y_test)","block_group":"b2b332f1016842ec90ab8e70c26d61b3","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"463898068bb44efe9deb6c62e255723e","deepnote_cell_type":"markdown"},"source":"## How does the algorithm affects the observation to a specific class ? ","block_group":"737a331cee79472886bcde88f66f5013"},{"cell_type":"code","metadata":{"cell_id":"28aa901220904543bd66001fab3f5157","deepnote_cell_type":"code"},"source":"#figbar, (ax1,ax2) = plt.subplots(nrows=2, ncols = 1,figsize = (20,12))\nbarwidth = 0.5\nfigbar, ax1 = plt.subplots(nrows=1, ncols = 1,figsize = (20,5))\n#ax1.tick_params(labelsize=18)\n#ax2.tick_params(labelsize=18)\n\nr1 = range(len(proba_log[:,0]))\nr2 = [x + barwidth for x in r1]\nax1.bar(r1, proba_log[:,0], width = barwidth, color=[color_class[0] for i in r1])\nax1.bar(r2, proba_log[:,1], width = barwidth, color=[color_class[1] for i in r1])\nax1.plot([0,len(proba_log[:,0])],[0.5,0.5])\nax1.set_title(r'Predicted probabilities',fontsize = 20)\nplt.xticks([r + barwidth / 2 for r in range(len(proba_log[:,0]))],[i for i in range(len(proba_log[:,0]),100)])\n#ax2.set_title(r'Predicted probabilities of class $1$',fontsize = 20)\nax1.set_xlabel(r'test set points $X$',fontsize = 20)\n#ax2.set_xlabel(r'test set points $X$',fontsize = 20)\n\n#plt.tight_layout()\nplt.show()","block_group":"7090cae7af7642c9a7a938f8211d5ac1","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"5a1c83c086b34ffdb41cf6c56881a426","deepnote_cell_type":"markdown"},"source":"## Now draw the decision line","block_group":"09a4537c75f84908be1226a226e68e78"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"ba7f3d2628cb45a8a8656c87c382000d","deepnote_cell_type":"code"},"source":"# Display\nh = 0.01  # step size in the mesh\nname = 'logistic regression'\nFigure, (ax,ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (14,5))\nax.set_xlabel(r'$x_1$',fontsize = 16)\nax.set_ylabel(r'$x_2$',fontsize = 16)\nax2.set_xlabel(r'$x_1$',fontsize = 16)\nax2.set_ylabel(r'$x_2$',fontsize = 16)\nax2.set_xlim(xmin=-1,xmax=2)\nax2.set_ylim(ymin=0,ymax=1)\n\n# Retrieve the model parameters for the decision line.\nb = logreg.intercept_[0]\nw1, w2 = logreg.coef_.T\n# Calculate the intercept and gradient of the decision boundary.\nc = -b/w2\nm = -w1/w2\n\n# Plot the data and the classification with the decision boundary.\nxmin, xmax = -1, 2\nymin, ymax = 0, 1\nxd = np.array([xmin, xmax])\nyd = m*xd + c\nax2.plot(xd, yd, color = 'black', linewidth = 1, linestyle = '--', label = 'decision line')\n\n\nDecisionBoundaryDisplay.from_estimator(\n        logreg,\n        X,\n        cmap=cmap_light_b,\n        alpha=0.3,\n        ax=ax,\n        response_method=\"predict\",\n        plot_method=\"pcolormesh\",\n        shading=\"auto\"\n    )\n\n# Plot also  testing points\nax.scatter(X0_t[:,0],X0_t[:,1], marker = '.', color=color_class[0])\nax.scatter(X1_t[:,0],X1_t[:,1], marker = '^', color=color_class[1])\n\nax.set_title(\"{} ({})\".format(name, 'ovr'))\n\nax.text(\n        0.8,\n        0.1,\n        \"Score: {:.2f}\".format(f1_score(y_test, y_pred)),\n        size=15,\n        ha=\"center\",\n        va=\"center\",\n        transform=ax.transAxes,\n)\n\nrect=Rectangle((-1, 0), 3, 1, \n    facecolor = None,\n    fill = None,\n    edgecolor = \"black\",\n    linewidth=0.5)\nax.add_patch(rect)\n\nDecisionBoundaryDisplay.from_estimator(\n        logreg,\n        X,\n        cmap=cmap_light_b,\n        alpha=0.3,\n        ax=ax2,\n        response_method=\"predict\",\n        plot_method=\"pcolormesh\",\n        shading=\"auto\"\n    )\n\n# Plot also  testing points\nax2.scatter(X0_t[:,0],X0_t[:,1], marker = '.', color='red')\nax2.scatter(X1_t[:,0],X1_t[:,1], marker = '^', color='blue')\nax2.set_title(\"Logistic regression: true test classes\")\nax2.legend(fontsize = 16)","block_group":"91129877960a477390e5d66b4cf51534","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"ecc076602b05400e893084e1d5627c52","deepnote_cell_type":"markdown"},"source":"## Suport Vector Machines (SVM)","block_group":"affa6fd9812f439685ac333915db7d08"},{"cell_type":"code","metadata":{"cell_id":"a13c3db821c34d239c6542e8b60f0a4d","deepnote_cell_type":"code"},"source":"from sklearn.svm import SVC","block_group":"e2c166ee44984ac2b5c32fbbe23ed284","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"98e48b6c57144ce1935c8e57aaca314f","deepnote_cell_type":"code"},"source":"# Create and train the SVM model\nsvmlin = SVC(kernel='linear') # 'linear' kernel is used here. \nsvmlin.fit(X_train, y_train)\n\n# Make predictions and evaluate the model\ny_pred = svmlin.predict(X_test)\n\nTest_model_accuracy (y_pred, y_test)","block_group":"347f35eebe414dcf852689a39700b6a7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"11edd719ab6d4121a43b1b4b0b8e7213","deepnote_cell_type":"code"},"source":"# Display\nh = 0.01  # step size in the mesh\nname = 'Suport Vector Machine'\nFigure, ax = plt.subplots()\nax.set_xlabel(r'$x_1$',fontsize = 16)\nax.set_ylabel(r'$x_2$',fontsize = 16)\n\nDecisionBoundaryDisplay.from_estimator(\n        svmlin,\n        X,\n        cmap=cmap_light_b,\n        alpha=0.3,\n        ax=ax,\n        response_method=\"predict\",\n        plot_method=\"pcolormesh\",\n        shading=\"auto\",\n    )\n\n# Plot also  testing points\n#plt.scatter(X_test[:, 0], X_[:, 1], c=y, cmap=cmap_bold, edgecolor=\"k\", s=20)\nax.scatter(X0_t[:,0],X0_t[:,1], marker = '.', color='red')\nax.scatter(X1_t[:,0],X1_t[:,1], marker = '^', color='blue')\n\nplt.title(\"{} ({})\".format(name, \"linear\"))\nplt.text(\n        0.9,\n        0.1,\n        \"{:.2f}\".format(f1_score(y_test, y_pred)),\n        size=15,\n        ha=\"center\",\n        va=\"center\",\n        transform=plt.gca().transAxes,\n)\n","block_group":"2836986f66104b86b292feabc493f491","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"f15a6df846e14454bc5996168faeb1a5","deepnote_cell_type":"markdown"},"source":"## Is there several SVM models ?","block_group":"0a195c7cfae544298e078e6cb2a05046"},{"cell_type":"code","metadata":{"cell_id":"49d0fc5901ea4e6895a430f7556e2a55","deepnote_cell_type":"code"},"source":"# The list of possible kernels\n#\nkernels = ['linear', 'poly','rbf','sigmoid']\n\nsvms=[]\npredict_svms=[]\naccuracy_svms = []\n\n# Create and train for each SVM model\n#\nfor k in kernels:\n    svm = SVC(kernel=k) # 'lbf' kernel is used here. \n    svm.fit(X_train, y_train)\n    svms.append(svm)\n    # Make predictions and evaluate the model\n    y_pred = svm.predict(X_test)\n    predict_svms.append(y_pred) \n    accuracy_svms.append(f1_score(y_pred, y_test))\n\nplt.plot(kernels,accuracy_svms)\nplt.xticks(fontsize = 16)\nplt.yticks(fontsize = 16)\nplt.xlabel(r'Kernel',fontsize = 16)\nplt.ylabel(r'accuracy',fontsize = 16)\nplt.show()","block_group":"a5029f0b870e4ccf8f293ecf07fd60fc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"f94d7273cac14888897693ae2bdbda72","deepnote_cell_type":"code"},"source":"# Display\nh = 0.05  # step size in the mesh\nname = 'Suport Vector Machine'\n\nnrows = 1\nncols = 4\nFigure, axs = plt.subplots(nrows = nrows, ncols = ncols, figsize = (25,6))\nfor k, i in zip(kernels,range(len(kernels))):\n    plt.sca(axs[i])\n    axs[i].set_xlabel(r'$x_1$',fontsize = 20)\n    axs[i].set_ylabel(r'$x_2$',fontsize = 20)\n    axs[i].tick_params(labelsize=20)\n\n    DecisionBoundaryDisplay.from_estimator(\n        svms[kernels.index(k)],\n        X,\n        cmap=cmap_light_b,\n        alpha=0.8,\n        ax=axs[i],\n        response_method=\"predict\",\n        plot_method=\"pcolormesh\",\n        shading=\"auto\",\n    )\n\n# Plot also  testing points\n#plt.scatter(X_test[:, 0], X_[:, 1], c=y, cmap=cmap_bold, edgecolor=\"k\", s=20)\n    axs[i].scatter(X0_t[:,0],X0_t[:,1], marker = '.', color='red')\n    axs[i].scatter(X1_t[:,0],X1_t[:,1], marker = '^', color='blue')\n\n    axs[i].set_title(\"{} ({})\".format(name, k),fontsize=20)\n    axs[i].text(\n        0.9,\n        0.1,\n        \"{:.2f}\".format(accuracy_svms[kernels.index(k)]),\n        size=15,\n        ha=\"center\",\n        va=\"center\",\n        transform=plt.gca().transAxes,\n    )\n\nplt.tight_layout()","block_group":"a05f51194a014436a5dffacb201a3703","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"436a278e1c0f4f2a9d8724ba4d908f72","deepnote_cell_type":"markdown"},"source":"## $k$-Nearest Neighbors ($k$-NN)","block_group":"689459f995584580bb42a38b9c510902"},{"cell_type":"code","metadata":{"cell_id":"7a01c22dc4a44cc3a451663008ece786","deepnote_cell_type":"code"},"source":"from sklearn.neighbors import KNeighborsClassifier\n\n#create the model classifier, the simplest one with 1 neighbor\n#\nn_neighbors = 1\nknn = KNeighborsClassifier(n_neighbors=n_neighbors)\n\n# train the model \nknn.fit(X_train, y_train) \n\n# Test the modelknn_clf=KNeighborsClassifier()\ny_pred = knn.predict(X_test)\nTest_model_accuracy (y_pred, y_test)\n\n","block_group":"878c8b889b9a464fa8bab1c91b9af95c","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"b63b4ec5d96045199cdf0105e1303075","deepnote_cell_type":"code"},"source":"# Display\nh = 0.05  # step size in the mesh\nname = 'k-NN'\nFigure, ax = plt.subplots()\nax.set_xlabel(r'$x_1$',fontsize = 16)\nax.set_ylabel(r'$x_2$',fontsize = 16)\n\nDecisionBoundaryDisplay.from_estimator(\n        knn,\n        X,\n        cmap=cmap_light_b,\n        alpha=0.3,\n        ax=ax,\n        response_method=\"predict\",\n        plot_method=\"pcolormesh\",\n        shading=\"auto\",\n    )\n\n# Plot also  test points according to their true class\nax.scatter(X0_t[:,0],X0_t[:,1], marker = '.', color='red')\nax.scatter(X1_t[:,0],X1_t[:,1], marker = '^', color='blue')\n\nplt.title(\"{} (k = {})\".format(name, n_neighbors))\nplt.text(\n        0.8,\n        0.1,\n        \"Score: {:.2f}\".format(f1_score(y_test, y_pred)),\n        size=15,\n        ha=\"center\",\n        va=\"center\",\n        transform=ax.transAxes,\n)\n","block_group":"7ef1c477584143239a285b8c9174e218","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"efa2a0eaa2234b22ac85f50bdf2f3df0","deepnote_cell_type":"markdown"},"source":"### Analysis\n* The decision line is close to the SVM model with the radial basis kernel (RBF) not too far from the dataset\n* A closer look shows that probably with $k=1$ ($1$ neighbor follows too closely the local environment of the training points: we find Class $1$ inside the region where we expect Class 0 points. **This is known as overfitting** \n* $k=1$ does not contain enough information of the neighborhood of the points : this means that we have to find a better model","block_group":"d04bd05d325047cdb7b1fc0d6e798cbe"},{"cell_type":"markdown","metadata":{"cell_id":"c69282011f23470c95ca4e2f25305ca4","deepnote_cell_type":"markdown"},"source":"## Model Selection\n\nHere we present a crude, yet comprehensive way of doing model selection. \n\nThe goal is to find the best value of $k$ that give the highest accuracy score","block_group":"48ed79fb1ae743b59cb8b02cf8b7df15"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"bac2a36be32448a9bbcef3b49dd3c51a","deepnote_cell_type":"code"},"source":"neighbors = np.arange(1, 30)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over K values\nfor i, k in enumerate(neighbors):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    \n    # Compute training and test data accuracy\n#    train_accuracy[i] = knn.score(X_train, y_train)\n    train_accuracy[i] = f1_score(y_train, knn.predict(X_train))\n    test_accuracy[i] = f1_score(y_test, knn.predict(X_test))\n\n# Generate plot\nplt.plot(neighbors, test_accuracy, label = 'Testing set Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training set Accuracy')\n\nplt.legend()\nplt.xlabel('n_neighbors')\nplt.ylabel('Accuracy')\nplt.show()\n","block_group":"71939bf3a5414f3e9468ea04f0a253a5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"c17e2fb947a3436488debd5ecac53ac5","deepnote_cell_type":"code"},"source":"# Select the model with the maximum accuracy and train it again\n#\nn_neighbors = np.argmax(test_accuracy)+1\nknn = KNeighborsClassifier(n_neighbors=n_neighbors)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)","block_group":"dabd00fbb16440da8d92fcd3bc96391a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"d51cc4cd84384ab9b1308b85a8a4f0e3","deepnote_cell_type":"code"},"source":"proba= knn.predict_proba(X_test)\n\nbarwidth = 0.5\nfigbar, ax1 = plt.subplots(nrows=1, ncols = 1,figsize = (20,5))\n#ax1.tick_params(labelsize=18)\n#ax2.tick_params(labelsize=18)\n\nr1 = range(len(proba_log[:,0]))\nr2 = [x + barwidth for x in r1]\nax1.bar(r1, proba_log[:,0], width = barwidth, color=[color_class[0] for i in r1])\nax1.bar(r2, proba_log[:,1], width = barwidth, color=[color_class[1]for i in r1])\nax1.plot([0,len(proba_log[:,0])],[0.5,0.5])\nax1.set_title(r'Predicted probabilities',fontsize = 20)\nplt.xticks([r + barwidth / 2 for r in range(len(proba_log[:,0]))],[i for i in range(len(proba_log[:,0]),100)])\n#ax2.set_title(r'Predicted probabilities of class $1$',fontsize = 20)\nax1.set_xlabel(r'test set points $X$',fontsize = 20)","block_group":"6bff3da3650e42c2aba6ac6e8f0e089e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"5c8371896fd04106b873a210376cc52d","deepnote_cell_type":"code"},"source":"# Display\nh = 0.05  # step size in the mesh\nname = 'k-NN'\nFigure, (ax,ax2,ax3) = plt.subplots(nrows = 1, ncols = 3, figsize = (21,5))\nax.set_xlabel(r'$x_1$',fontsize = 16)\nax.set_ylabel(r'$x_2$',fontsize = 16)\n\nax2.set_xlabel(r'$x_1$',fontsize = 16)\nax2.set_ylabel(r'$x_2$',fontsize = 16)\nax2.set_xlim(xmin=-1,xmax=2)\nax2.set_ylim(ymin=0,ymax=1)\n\nax3.set_xlabel(r'$x_1$',fontsize = 16)\nax3.set_ylabel(r'$x_2$',fontsize = 16)\nax3.set_xlim(xmin=-1,xmax=2)\nax3.set_ylim(ymin=0,ymax=1)\n\nDecisionBoundaryDisplay.from_estimator(\n        knn,\n        X,\n        cmap=cmap_light_b,\n        alpha=0.3,\n        ax=ax2,\n        response_method=\"predict\",\n        plot_method=\"pcolormesh\",\n        shading=\"auto\",\n    )\n\nax2.set_title(\"{} (k = {}) predicted classes on the test\".format(name, n_neighbors))\nax2.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap=cmap1, edgecolor=\"k\", s=60)\nax2.text(\n        0.8,\n        0.1,\n        \"score {:.2f}\".format(f1_score(y_test, y_pred)),\n        size=15,\n        ha=\"center\",\n        va=\"center\",\n        transform=ax2.transAxes,\n)\n\nDecisionBoundaryDisplay.from_estimator(\n        knn,\n        X,\n        cmap=cmap_light_b,\n        alpha=0.3,\n        ax=ax3,\n        response_method=\"predict\",\n        plot_method=\"pcolormesh\",\n        shading=\"auto\",\n    )\n# Plot also testing points\n#plt.scatter(X_test[:, 0], X_[:, 1], c=y, cmap=cmap_bold, edgecolor=\"k\", s=20)\n\nax3.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap1, edgecolor=\"k\", s=60)\n\nax3.set_title(\"{} (k = {}) true test classes\".format(name, n_neighbors))\n\nDecisionBoundaryDisplay.from_estimator(\n        knn,\n        X,\n        cmap=cmap_light_b,\n        alpha=0.3,\n        ax=ax,\n        response_method=\"predict\",\n        plot_method=\"pcolormesh\",\n        shading=\"auto\",\n    )\n\nrect=Rectangle((-1, 0), 3, 1, \n    facecolor = None,\n    fill = None,\n    edgecolor = \"black\",\n    linewidth=0.5)\n\nax.add_patch(rect)\n\nax.set_title(\"Predicted probabilities of class 0\")\n\n# Plot also  testing points\n#mycolors = cmap1(np.linspace(0,1,len(X_test)))\ncm = ax.scatter(X_test[:,0],X_test[:,1], c = proba[:,1],cmap = cmap1)\nFigure.colorbar(cm, ax=ax)","block_group":"917343c38da749428237e6add8dc13ec","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"212c45f51679476eb9177bb6fbf81f4c","deepnote_cell_type":"markdown"},"source":"# Multi-class classification","block_group":"69d3c22458e84370862326f3f59fe6cb"},{"cell_type":"code","metadata":{"cell_id":"afdc9dd3f7cd4734b17254aca2509275","deepnote_cell_type":"code"},"source":"# test classification dataset\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\n\n# define the random state\nrandom_state = 1\n\n# define the number of classes\nn_classes = 3\n\n# define dataset\nXm, ym = make_classification(n_samples=1000\n                           , n_features=2\n                           , n_informative=2\n                           , n_redundant=0\n                           , n_classes= n_classes\n                           , n_clusters_per_class = 1\n                           , class_sep = 1.7\n                           , random_state=random_state)\n# summarize the dataset\nprint(Xm.shape, ym.shape)\nprint(Counter(ym))","block_group":"331730ab3db84b9fba8d47358c3ff418","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"b46afa3ae15d4f6ab804fde18f6f01ec","deepnote_cell_type":"code"},"source":"Figure, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\nax1.scatter(Xm[:,0],Xm[:,1])\nax1.set_xlabel(r'$x_1$',fontsize = 16)\nax1.set_ylabel(r'$x_2$',fontsize = 16)\n\nX0m_s, X1m_s, X2m_s, = Xm[ym == 0], Xm[ym == 1], Xm[ym == 2]\nax2.scatter(X0m_s[:,0],X0m_s[:,1], marker = '.', color = color_class[0], alpha=0.5)\nax2.scatter(X1m_s[:,0],X1m_s[:,1], marker = '^', color = color_class[1], alpha=0.5)\nax2.scatter(X2m_s[:,0],X2m_s[:,1], marker = '^', color = color_class[2], alpha=0.5)\nax2.set_xlabel(r'$x_1$',fontsize = 16)\nax2.set_ylabel(r'$x_2$',fontsize = 16)\nplt.show()","block_group":"b22bed9e77e44a5a8a5be1c39e5f2089","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"c56b37c065e7402ea24c200ca807f76c","deepnote_cell_type":"markdown"},"source":"## Logistic regression","block_group":"2b355a5ddffd471f88482c8929db43eb"},{"cell_type":"code","metadata":{"cell_id":"3bcb87a9a4844c649848bec01e6f1ad6","deepnote_cell_type":"code"},"source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.inspection import DecisionBoundaryDisplay","block_group":"439fd151dddc4ac98cebd172503ff7d3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"4b4e2705c66541469dee1e06d822cb61","deepnote_cell_type":"code"},"source":"# define the multinomial logistic regression model\nmlogreg = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n\n# define the model evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n# evaluate the model and collect the scores\nn_scores = cross_val_score(mlogreg, Xm, ym, scoring='accuracy', cv=cv, n_jobs=-1)\n\n# report the model performance\nprint('Mean Accuracy: %.3f (+/- %.3f)' % (mean(n_scores), std(n_scores)))","block_group":"fb1294751ef84045b84ea7c91886f3f8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"be6fa39b72e7432eb0811969b82af4e1","deepnote_cell_type":"code"},"source":"# split the dataset into a train set and a test set : usually 80% of the data in the training\nXm_train, Xm_test, ym_train, ym_test = train_test_split(Xm, ym, test_size=0.20, random_state=random_state)","block_group":"e0c8843d88be461bb6207265268a6160","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"aaf00a0c8ebf4cf3978ffff1f145b3b0","deepnote_cell_type":"code"},"source":"mlogreg.fit(Xm_train, ym_train)\nym_pred = mlogreg.predict(Xm_test)\nproba_log = mlogreg.predict_proba(Xm_test)\n# separate instances with classes 0 and 1 for the drawing\nX0m_tr, X1m_tr, X2m_tr = Xm_train[ym_train == 0], Xm_train[ym_train == 1], Xm_train[ym_train == 2]\nX0m_t, X1m_t, X2m_t = Xm_test[ym_test == 0], Xm_test[ym_test == 1], Xm_test[ym_test == 2]\n","block_group":"45e91831cbcb43ce80134229fcd8f1d6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"713aca5c788549c082892b2e81a8e0e1","deepnote_cell_type":"code"},"source":"h = 0.05  # step size in the mesh\nname = 'multiclass Logistic regressions'\nFigure, ax = plt.subplots()\nax.set_xlabel(r'$x_1$',fontsize = 16)\nax.set_ylabel(r'$x_2$',fontsize = 16)\n\nDecisionBoundaryDisplay.from_estimator(\n        mlogreg,\n        Xm,\n        cmap=cmap_light,\n        alpha=0.3,\n        ax=ax,\n        response_method=\"predict\",\n        plot_method=\"pcolormesh\",\n        shading=\"auto\",\n    )\n\n# Plot also  test points according to their true class\nplt.scatter(Xm_test[:, 0], Xm_test[:, 1], c=ym_test, cmap=cmap_bold, edgecolor=\"k\", s=20)\n\nplt.title(\"{} (nb classes = {})\".format(name, n_classes))\nplt.text(\n        0.8,\n        0.1,\n        \"Score: {:.2f}\".format(mean(n_scores)),\n        size=15,\n        ha=\"center\",\n        va=\"center\",\n        transform=ax.transAxes,\n)","block_group":"e28bffc0da8b421882c53da007a23edd","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"ef011a24b26047f49a75aa95be60ee0d","deepnote_cell_type":"code"},"source":"# Choose a point and predict its class\npoint = [[2, -2]]\npoint_class = mlogreg.predict(point)","block_group":"127ef2b667db4011b72381c716c82be6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"771e99010f7a457e8609b0c9c188b559","deepnote_cell_type":"code"},"source":"Figure, ax = plt.subplots()\nax.set_xlabel(r'$x_1$',fontsize = 16)\nax.set_ylabel(r'$x_2$',fontsize = 16)\n\nDecisionBoundaryDisplay.from_estimator(\n        mlogreg,\n        Xm,\n        cmap=cmap_light,\n        alpha=0.3,\n        ax=ax,\n        response_method=\"predict\",\n        plot_method=\"pcolormesh\",\n        shading=\"auto\",\n    )\n\nax.scatter(point[0][0],point[0][1], marker = '*', s=400, c = color_class[int(point_class)], edgecolor=\"yellow\")","block_group":"31f403c76e7341efb7a2a53bfeb9f26a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"bb1b30c3f83c4a45b22d2e8ab89602de","deepnote_cell_type":"markdown"},"source":"## Support Vector Machines","block_group":"5aea734e47f046f7ba6b38811b9caa4e"},{"cell_type":"code","metadata":{"cell_id":"52406a9ca3c841668076d5052afb534d","deepnote_cell_type":"code"},"source":"kernels = ['linear', 'poly','rbf','sigmoid']\n\nsvms=[]\npredict_svms=[]\naccuracy_svms = []\n\n# Create and train for each SVM model\n#\nfor k in kernels:\n    svm = SVC(kernel=k) # 'lbf' kernel is used here. \n    svm.fit(Xm_train, ym_train)\n    svms.append(svm)\n    # Make predictions and evaluate the model\n    ym_pred = svm.predict(Xm_test)\n    predict_svms.append(ym_pred) \n    n_scores = cross_val_score(svm, Xm, ym, scoring='accuracy', cv=cv, n_jobs=-1)\n    accuracy_svms.append(mean(n_scores))\n\nplt.scatter(kernels,accuracy_svms)\nplt.errorbar(kernels, accuracy_svms, yerr=std(n_scores), capsize=5)\nplt.xticks(fontsize = 16)\nplt.yticks(fontsize = 16)\nplt.xlabel(r'Kernel',fontsize = 16)\nplt.ylabel(r'accuracy',fontsize = 16)\nplt.show()","block_group":"2be756454d054d9f84ccf3023b4bb70d","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"acb597c5c334403d963ee2d1312a2572","deepnote_cell_type":"code"},"source":"# Display\nh = 0.05  # step size in the mesh\nname = 'Suport Vector Machine'\n\nnrows = 1\nncols = 4\nFigure, axs = plt.subplots(nrows = nrows, ncols = ncols, figsize = (25,6))\nfor k, i in zip(kernels,range(len(kernels))):\n    plt.sca(axs[i])\n    axs[i].set_xlabel(r'$x_1$',fontsize = 20)\n    axs[i].set_ylabel(r'$x_2$',fontsize = 20)\n    axs[i].tick_params(labelsize=20)\n\n    DecisionBoundaryDisplay.from_estimator(\n        svms[kernels.index(k)],\n        Xm,\n        cmap=cmap_light,\n        alpha=0.3,\n        ax=axs[i],\n        response_method=\"predict\",\n        plot_method=\"pcolormesh\",\n        shading=\"auto\",\n    )\n\n# Plot also  testing points\n    axs[i].scatter(Xm_test[:, 0], Xm_test[:, 1], c=ym_pred, cmap=cmap_bold, edgecolor=\"k\", s=20)\n    axs[i].set_title(\"{} ({})\".format(name, k),fontsize=20)\n    axs[i].text(\n        0.9,\n        0.1,\n        \"{:.2f}\".format(accuracy_svms[kernels.index(k)]),\n        size=15,\n        ha=\"center\",\n        va=\"center\",\n        transform=plt.gca().transAxes,\n    )\n\nplt.tight_layout()","block_group":"eedfd1a38bc246e9bb4a6bcd575181ea","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"a79091c0d5514ae6920e83f32767b2b6","deepnote_cell_type":"markdown"},"source":"### K-NN with $k=1$","block_group":"fc98a039ec234d49bb2a086edbbe317e"},{"cell_type":"code","metadata":{"cell_id":"1ed6fdabd8424fd4afb3285de8c9a4bd","deepnote_cell_type":"code"},"source":"from sklearn.neighbors import KNeighborsClassifier\n\n#create the model classifier, the simplest one with 1 neighbor\n#\nn_neighbors = 1\nknn1 = KNeighborsClassifier(n_neighbors=n_neighbors)\n\n# train the model \nknn1.fit(Xm_train, ym_train) \n\n# Test the modelknn_clf=KNeighborsClassifier()\nym_pred = knn1.predict(Xm_test)","block_group":"c0fc42d0cefc4f8c89e3fea2f2dcae1b","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"1ae7ad251d58462f83834cb0205782e8","deepnote_cell_type":"code"},"source":"# Display\nh = 0.05  # step size in the mesh\nname = 'k-NN'\nFigure, ax = plt.subplots()\nax.set_xlabel(r'$x_1$',fontsize = 16)\nax.set_ylabel(r'$x_2$',fontsize = 16)\n\nDecisionBoundaryDisplay.from_estimator(\n        knn1,\n        Xm,\n        cmap=cmap_light,\n        alpha=0.3,\n        ax=ax,\n        response_method=\"predict\",\n        plot_method=\"pcolormesh\",\n        shading=\"auto\",\n    )\n\nplt.scatter(Xm_test[:, 0], Xm_test[:, 1], c=ym_test, cmap=cmap_bold, edgecolor=\"k\", s=20)\nplt.title(\"{} (k = {})\".format(name, n_neighbors))\nplt.text(\n        0.8,\n        0.1,\n        \"Score: {:.2f}\".format(f1_score(y_test, y_pred)),\n        size=15,\n        ha=\"center\",\n        va=\"center\",\n        transform=ax.transAxes,\n)\n","block_group":"804146e66ebe4dfe85fe23c509bba59a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"06969f346bc1428daafab4397523bbcb","deepnote_cell_type":"markdown"},"source":"## Exercise: make a svm classification with a dataset of 4 classes  ","block_group":"38498dbf738647c59bf68755bf08f413"},{"cell_type":"markdown","metadata":{"cell_id":"fce5fece3caf4007bfcaa4c34d5c6145","deepnote_cell_type":"markdown"},"source":"Here is the dataset that you are going to work on.\n","block_group":"c5b8d63b80fc41caa78154450decb721"},{"cell_type":"code","metadata":{"cell_id":"5ef99602a15645e6bd08eb6913d1332f","deepnote_cell_type":"code"},"source":"# test classification dataset\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\n\n# define the random state\nrandom_state = 1\n\n# define the number of classes\nn_classes = 4\n\n# define dataset\nXc, yc = make_classification(n_samples=3000\n                           , n_features=2\n                           , n_informative= 2\n                           , n_redundant=0\n                           , n_classes= n_classes\n                           , n_clusters_per_class = 1\n                           , class_sep = 1.7\n                           , random_state=random_state)\n# summarize the dataset\nprint(Xc.shape, yc.shape)\nprint(Counter(yc))\nprint(yc)","block_group":"786cf88d9f0b4c90b2de6eb4ce18dd6d","execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"(3000, 2) (3000,)\nCounter({1: 753, 2: 751, 3: 749, 0: 747})\n[2 2 3 ... 0 3 0]\n"}]},{"cell_type":"markdown","metadata":{"cell_id":"4ce7f134d3bf49b2995e2cff826154bd","deepnote_cell_type":"markdown"},"source":"Questions:\n1. Visualise the data, perform a train/test split \n2. train and evaluate the 4 different SMV models\n3. Draw the predictions (d√©cision lines)\n4. Which SVM model seems to be the most suitable for a classification ?","block_group":"149af155a8af472bb391aa0692ac1672"},{"cell_type":"markdown","metadata":{"cell_id":"ace46ea033fd495ab93a78859bb36626","deepnote_cell_type":"markdown"},"source":"### Visualise the data ","block_group":"a46de1c73782498d9d8e7a9092ca3941"},{"cell_type":"code","metadata":{"cell_id":"dd52deaa036b41418fdd5bce3ff4c317","deepnote_cell_type":"code"},"source":"#code","block_group":"4bd54e67a918411e9326237466496ad0","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"408db3191c964b26b99fa0f152cff8aa","deepnote_cell_type":"markdown"},"source":"### Train-test split","block_group":"d2a007ce15b94dc7a385f1a79ef6e46f"},{"cell_type":"code","metadata":{"cell_id":"514029c39c8148eb9123e96933b1a09c","deepnote_cell_type":"code"},"source":"#code","block_group":"a5005bd614f54f0aac6ecde084ff09d9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"80a02ab62da44e85a46fa231d934ba0a","deepnote_cell_type":"markdown"},"source":"### Train the models","block_group":"82657a31e2b14a429bc6b2286814fd5c"},{"cell_type":"code","metadata":{"cell_id":"118fa41778e947158978ecc2cf8196e7","deepnote_cell_type":"code"},"source":"#code","block_group":"24ad11c3881e408dad3845899d2de6b3","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"f5c378eee2e048d3af7aa01cd73f2f4a","deepnote_cell_type":"markdown"},"source":"### Visualize the predictions","block_group":"ed571b7872ff419a91818104ffd27022"},{"cell_type":"code","metadata":{"cell_id":"be670ff9a58f4c99ad82f73361398d28","deepnote_cell_type":"code"},"source":"#code","block_group":"96b578977c1446539b3f247443331206","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=fe3254e6-9d62-4c8c-aa95-7472e9779ff6' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3 (ipykernel)"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"2d3e4cbffcf144f4aa60f1052f3ea6a5","deepnote_execution_queue":[]}}